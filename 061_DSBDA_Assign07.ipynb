{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b8307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e43734",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('07_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11632763",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b55516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9150d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuations\n",
    "\n",
    "data['punctremov'] = ''\n",
    "\n",
    "for i in range(0, 5574):\n",
    "    data['punctremov'][i] = data['sms'][i].translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae7519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90412c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case \n",
    "\n",
    "data['sms_lower'] = ''\n",
    "\n",
    "for i in range(0, 5574):\n",
    "    data['sms_lower'][i] = data['punctremov'][i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7deaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "\n",
    "data['sms_token'] = ''\n",
    "\n",
    "for i in range(0, 5574):\n",
    "    data['sms_token'][i] = word_tokenize(data['sms_lower'][i])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e3c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of stop words\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4846b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords\n",
    "\n",
    "data['sms_nostop'] = ''\n",
    "\n",
    "for i in range(0, 5574):\n",
    "    filtered_sent = []\n",
    "    for w in data['sms_token'][i]:\n",
    "        if w not in stop_words:\n",
    "            filtered_sent.append(w)\n",
    "    data['sms_nostop'][i] = filtered_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd314c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "\n",
    "data['sms_stem'] = ''\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range(0, 5574):\n",
    "    stemmed_words = []\n",
    "    for w in data['sms_nostop'][i]:\n",
    "        stemmed_words.append(ps.stem(w))\n",
    "    data['sms_stem'][i] = stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c51d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sms_stem'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ba852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "data['sms_lemmat'] = ''\n",
    "\n",
    "for i in range (5574):\n",
    "    lemmatized_words = []\n",
    "    for w in data['sms_nostop'][i]:\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(w))\n",
    "    data['sms_lemmat'][i] = lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b3258",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sms_lemmat'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be76fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging \n",
    "\n",
    "data['POS'] = ''\n",
    "\n",
    "for i in range(0, 5574):\n",
    "    result = nltk.pos_tag(data['sms_nostop'][i])\n",
    "    data['POS'][i] = result\n",
    "\n",
    "data['POS'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "\n",
    "N = len(data)\n",
    "def doc_freq(word):\n",
    "    df = sum(1 for text in data['sms_stem'] if word in text)\n",
    "    return df\n",
    "\n",
    "def calc_tf_idf(doc, token):\n",
    "    words_count = len(data['sms_stem'][doc])\n",
    "    counter = dict(nltk.FreqDist(data['sms_stem'][doc]))\n",
    "    tf = counter[token]/words_count\n",
    "    df = doc_freq(token)\n",
    "    idf = np.log(N/(df+1))\n",
    "    tf_idf = tf*idf\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba753239",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data['sms_stem'][0]:\n",
    "  tf_idf = calc_tf_idf(doc=0, token=i)\n",
    "  print(i,\" \", tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb58c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
